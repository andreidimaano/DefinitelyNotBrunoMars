{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "from model import Generator, Discriminator\n",
    "# from args.cycleGAN_train_arg_parser import CycleGANTrainArgParser\n",
    "from VCDataset import VCDataset\n",
    "import h5py\n",
    "import time\n",
    "# from mask_cyclegan_vc.utils import decode_melspectrogram, get_mel_spectrogram_fig\n",
    "# from logger.train_logger import TrainLogger\n",
    "# from saver.model_saver import ModelSaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda\")\n",
    "#     # Set the device for tensors\n",
    "#     torch.cuda.set_device(device)\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "\n",
    "# generator_lr = 0.0002\n",
    "# generator_A2B = Generator().to(device)\n",
    "# generator_B2A = Generator().to(device)\n",
    "\n",
    "#     # Initialize Optimizers\n",
    "# g_params = list(generator_A2B.parameters()) + \\\n",
    "#     list(generator_B2A.parameters())\n",
    "# generator_optimizer = torch.optim.Adam(\n",
    "#     g_params, lr=generator_lr, betas=(0.5, 0.999))\n",
    "\n",
    "# for i, (real_A, mask_A, real_B, mask_B) in enumerate(train_dataloader):\n",
    "\n",
    "#     with torch.set_grad_enabled(True):\n",
    "#         real_A = real_A.to(device, dtype=torch.float)\n",
    "#         mask_A = mask_A.to(device, dtype=torch.float)\n",
    "#         real_B = real_B.to(device, dtype=torch.float)\n",
    "#         mask_B = mask_B.to(device, dtype=torch.float)\n",
    "\n",
    "#     # ----------------\n",
    "#     # Train Generator\n",
    "#     # ----------------\n",
    "#     generator_A2B.train()\n",
    "#     generator_B2A.train()\n",
    "\n",
    "#     fake_B = generator_A2B(real_A, mask_A)\n",
    "#     cycle_A = generator_B2A(fake_B, torch.ones_like(fake_B))\n",
    "#     fake_A = generator_B2A(real_B, mask_B)\n",
    "#     cycle_B = generator_A2B(fake_A, torch.ones_like(fake_A))\n",
    "#     identity_A = generator_B2A(\n",
    "#         real_A, torch.ones_like(real_A))\n",
    "#     identity_B = generator_A2B(\n",
    "#         real_B, torch.ones_like(real_B))\n",
    "    \n",
    "#     # Generator Cycle Loss\n",
    "#     cycleLoss = torch.mean(torch.abs(real_A - cycle_A)) + torch.mean(torch.abs(real_B - cycle_B))\n",
    "#     # Generator Identity Loss\n",
    "#     identityLoss = torch.mean(torch.abs(real_A - identity_A)) + torch.mean(torch.abs(real_B - identity_B))\n",
    "\n",
    "#     # Generator Loss\n",
    "#     g_loss_A2B = torch.mean((1 - d_fake_B) ** 2)\n",
    "#     g_loss_B2A = torch.mean((1 - d_fake_A) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_file = h5py.File('iu_spec.h5', 'r')\n",
    "dataset_a = h5_file['audio']\n",
    "dataset_a = np.array(dataset_a)\n",
    "h5_file.close()\n",
    "\n",
    "h5_file = h5py.File('bruno_spec.h5', 'r')\n",
    "dataset_b = h5_file['audio']\n",
    "dataset_b = np.array(dataset_b)\n",
    "h5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22732, 80, 130) (2239, 80, 130)\n"
     ]
    }
   ],
   "source": [
    "print(dataset_a.shape, dataset_b.shape)\n",
    "dataset = VCDataset(datasetA_spec=dataset_a,\n",
    "                    datasetB_spec=dataset_b,\n",
    "                    n_frames=64, \n",
    "                    max_mask_len=25)\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                               batch_size=1,\n",
    "                                               shuffle=True,\n",
    "                                               drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trains MaskCycleGAN-VC as described in https://arxiv.org/pdf/2102.12841.pdf\n",
    "Inspired by https://github.com/jackaduma/CycleGAN-VC2\n",
    "\"\"\"\n",
    "\n",
    "class MaskCycleGANVCTraining(object):\n",
    "    \"\"\"Trainer for MaskCycleGAN-VC\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            args (Namespace): Program arguments from argparser\n",
    "        \"\"\"\n",
    "        # Store args\n",
    "        self.training_time = 0\n",
    "        self.average_time = 0\n",
    "        self.num_epochs = 10\n",
    "        self.start_epoch = 1\n",
    "        self.generator_lr = 1e-5\n",
    "        self.discriminator_lr = 1e-4\n",
    "        self.decay_after = 2e5\n",
    "        self.stop_identity_after = 1e4\n",
    "        self.mini_batch_size = 1\n",
    "        self.cycle_loss_lambda = 10\n",
    "        self.identity_loss_lambda = 5\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        self.epochs_per_save = 100\n",
    "        self.epochs_per_plot = 10\n",
    "\n",
    "        # Initialize MelGAN-Vocoder used to decode Mel-spectrograms\n",
    "        # self.vocoder = torch.hub.load('descriptinc/melgan-neurips', 'load_melgan')\n",
    "\n",
    "        self.sample_rate = 22050\n",
    "\n",
    "        h5_file = h5py.File('iu_spec.h5', 'r')\n",
    "        dataset_a = h5_file['audio']\n",
    "        self.datasetA_spec = np.array(dataset_a)\n",
    "        h5_file.close()\n",
    "\n",
    "        h5_file = h5py.File('bruno_spec.h5', 'r')\n",
    "        dataset_b = h5_file['audio']\n",
    "        self.datasetB_spec = np.array(dataset_b)\n",
    "        h5_file.close()\n",
    "        # Initialize speakerA's dataset\n",
    "        # self.datasetA = self.loadPickleFile(os.path.join(\n",
    "        #     args.preprocessed_data_dir, args.speaker_A_id, f\"{args.speaker_A_id}_normalized.pickle\"))\n",
    "        # datasetA_norm_stats = np.load(os.path.join(\n",
    "        #     args.preprocessed_data_dir, args.speaker_A_id, f\"{args.speaker_A_id}_norm_stat.npz\"))\n",
    "        # self.datasetA_spec_mean = datasetA_norm_stats['mean']\n",
    "        # self.datasetA_std = datasetA_norm_stats['std']\n",
    "\n",
    "        # Initialize speakerB's dataset\n",
    "        # self.dataset_B_spec = self.loadPickleFile(os.path.join(\n",
    "        #     args.preprocessed_data_dir, args.speaker_B_id, f\"{args.speaker_B_id}_normalized.pickle\"))\n",
    "        # dataset_B_norm_stats = np.load(os.path.join(\n",
    "        #     args.preprocessed_data_dir, args.speaker_B_id, f\"{args.speaker_B_id}_norm_stat.npz\"))\n",
    "        # self.dataset_B_spec_mean = dataset_B_norm_stats['mean']\n",
    "        # self.dataset_B_spec_std = dataset_B_norm_stats['std']\n",
    "\n",
    "        # Compute lr decay rate\n",
    "        self.n_samples = len(self.datasetA_spec)\n",
    "        print(f'n_samples = {self.n_samples}')\n",
    "        self.generator_lr_decay = self.generator_lr / \\\n",
    "            float(self.num_epochs * (self.n_samples // self.mini_batch_size))\n",
    "        self.discriminator_lr_decay = self.discriminator_lr / \\\n",
    "            float(self.num_epochs * (self.n_samples // self.mini_batch_size))\n",
    "        print(f'generator_lr_decay = {self.generator_lr_decay}')\n",
    "        print(f'discriminator_lr_decay = {self.discriminator_lr_decay}')\n",
    "\n",
    "        # Initialize Train Dataloader\n",
    "        self.num_frames = 64 \n",
    "        self.dataset = VCDataset(datasetA_spec=self.datasetA_spec,\n",
    "                    datasetB_spec=self.datasetB_spec,\n",
    "                    n_frames=64, \n",
    "                    max_mask_len=25)\n",
    "        self.train_dataloader = torch.utils.data.DataLoader(dataset=self.dataset,\n",
    "                                                            batch_size=self.mini_batch_size,\n",
    "                                                            shuffle=True,\n",
    "                                                            drop_last=False)\n",
    "\n",
    "        # Initialize Validation Dataloader (used to generate intermediate outputs)\n",
    "        self.validation_dataset = VCDataset(datasetA_spec=self.datasetA_spec,\n",
    "                                            datasetB_spec=self.datasetB_spec,\n",
    "                                            n_frames=320,\n",
    "                                            max_mask_len=32,\n",
    "                                            valid=True)\n",
    "        self.validation_dataloader = torch.utils.data.DataLoader(dataset=self.validation_dataset,\n",
    "                                                                 batch_size=1,\n",
    "                                                                 shuffle=False,\n",
    "                                                                 drop_last=False)\n",
    "\n",
    "        # Initialize logger and saver objects\n",
    "        # self.logger = TrainLogger(args, len(self.train_dataloader.dataset))\n",
    "        # self.saver = ModelSaver(args)\n",
    "\n",
    "        # Initialize Generators and Discriminators\n",
    "        self.generator_A2B = Generator().to(self.device)\n",
    "        self.generator_B2A = Generator().to(self.device)\n",
    "        self.discriminator_A = Discriminator().to(self.device)\n",
    "        self.discriminator_B = Discriminator().to(self.device)\n",
    "        # Discriminator to compute 2 step adversarial loss\n",
    "        self.discriminator_A2 = Discriminator().to(self.device)\n",
    "        # Discriminator to compute 2 step adversarial loss\n",
    "        self.discriminator_B2 = Discriminator().to(self.device)\n",
    "\n",
    "        # Initialize Optimizers\n",
    "        g_params = list(self.generator_A2B.parameters()) + \\\n",
    "            list(self.generator_B2A.parameters())\n",
    "        d_params = list(self.discriminator_A.parameters()) + \\\n",
    "            list(self.discriminator_B.parameters()) + \\\n",
    "            list(self.discriminator_A2.parameters()) + \\\n",
    "            list(self.discriminator_B2.parameters())\n",
    "        self.generator_optimizer = torch.optim.Adam(\n",
    "            g_params, lr=self.generator_lr, betas=(0.5, 0.999))\n",
    "        self.discriminator_optimizer = torch.optim.Adam(\n",
    "            d_params, lr=self.discriminator_lr, betas=(0.5, 0.999))\n",
    "\n",
    "        # Load from previous ckpt\n",
    "        # if args.continue_train:\n",
    "        #     self.saver.load_model(\n",
    "        #         self.generator_A2B, \"generator_A2B\", None, self.generator_optimizer)\n",
    "        #     self.saver.load_model(self.generator_B2A,\n",
    "        #                           \"generator_B2A\", None, None)\n",
    "        #     self.saver.load_model(self.discriminator_A,\n",
    "        #                           \"discriminator_A\", None, self.discriminator_optimizer)\n",
    "        #     self.saver.load_model(self.discriminator_B,\n",
    "        #                           \"discriminator_B\", None, None)\n",
    "        #     self.saver.load_model(self.discriminator_A2,\n",
    "        #                           \"discriminator_A2\", None, None)\n",
    "        #     self.saver.load_model(self.discriminator_B2,\n",
    "        #                           \"discriminator_B2\", None, None)\n",
    "\n",
    "    def adjust_lr_rate(self, optimizer, generator):\n",
    "        \"\"\"Decays learning rate.\n",
    "\n",
    "        Args:\n",
    "            optimizer (torch.optim): torch optimizer\n",
    "            generator (bool): Whether to adjust generator lr.\n",
    "        \"\"\"\n",
    "        if generator:\n",
    "            self.generator_lr = max(\n",
    "                0., self.generator_lr - self.generator_lr_decay)\n",
    "            for param_groups in optimizer.param_groups:\n",
    "                param_groups['lr'] = self.generator_lr\n",
    "        else:\n",
    "            self.discriminator_lr = max(\n",
    "                0., self.discriminator_lr - self.discriminator_lr_decay)\n",
    "            for param_groups in optimizer.param_groups:\n",
    "                param_groups['lr'] = self.discriminator_lr\n",
    "\n",
    "    def reset_grad(self):\n",
    "        \"\"\"Sets gradients of the generators and discriminators to zero before backpropagation.\n",
    "        \"\"\"\n",
    "        self.generator_optimizer.zero_grad()\n",
    "        self.discriminator_optimizer.zero_grad()\n",
    "\n",
    "    def loadPickleFile(self, fileName):\n",
    "        \"\"\"Loads a Pickle file.\n",
    "\n",
    "        Args:\n",
    "            fileName (str): pickle file path\n",
    "\n",
    "        Returns:\n",
    "            file object: The loaded pickle file object\n",
    "        \"\"\"\n",
    "        with open(fileName, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def get_training_time(self):\n",
    "        return self.training_time, self.average_time\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Implements the training loop for MaskCycleGAN-VC\n",
    "        \"\"\"\n",
    "        j = 0\n",
    "        start_time = time.time()\n",
    "        for epoch in range(self.start_epoch, self.num_epochs + 1):\n",
    "            # self.logger.start_epoch()\n",
    "            for i, (real_A, mask_A, real_B, mask_B) in enumerate(self.train_dataloader):\n",
    "                if j % 10 == 0:\n",
    "                    print(f'iteration {j} start')\n",
    "                # self.logger.start_iter()\n",
    "                num_iterations = (\n",
    "                    self.n_samples // self.mini_batch_size) * epoch + i\n",
    "\n",
    "                with torch.set_grad_enabled(True):\n",
    "                    real_A = real_A.to(self.device, dtype=torch.float)\n",
    "                    mask_A = mask_A.to(self.device, dtype=torch.float)\n",
    "                    real_B = real_B.to(self.device, dtype=torch.float)\n",
    "                    mask_B = mask_B.to(self.device, dtype=torch.float)\n",
    "\n",
    "                    # ----------------\n",
    "                    # Train Generator\n",
    "                    # ----------------\n",
    "                    self.generator_A2B.train()\n",
    "                    self.generator_B2A.train()\n",
    "                    self.discriminator_A.eval()\n",
    "                    self.discriminator_B.eval()\n",
    "                    self.discriminator_A2.eval()\n",
    "                    self.discriminator_B2.eval()\n",
    "\n",
    "                    # Generator Feed Forward\n",
    "                    fake_B = self.generator_A2B(real_A, mask_A)\n",
    "                    cycle_A = self.generator_B2A(fake_B, torch.ones_like(fake_B))\n",
    "                    fake_A = self.generator_B2A(real_B, mask_B)\n",
    "                    cycle_B = self.generator_A2B(fake_A, torch.ones_like(fake_A))\n",
    "                    identity_A = self.generator_B2A(\n",
    "                        real_A, torch.ones_like(real_A))\n",
    "                    identity_B = self.generator_A2B(\n",
    "                        real_B, torch.ones_like(real_B))\n",
    "                    d_fake_A = self.discriminator_A(fake_A)\n",
    "                    d_fake_B = self.discriminator_B(fake_B)\n",
    "\n",
    "                    # For Two Step Adverserial Loss\n",
    "                    d_fake_cycle_A = self.discriminator_A2(cycle_A)\n",
    "                    d_fake_cycle_B = self.discriminator_B2(cycle_B)\n",
    "\n",
    "                    # Generator Cycle Loss\n",
    "                    cycleLoss = torch.mean(\n",
    "                        torch.abs(real_A - cycle_A)) + torch.mean(torch.abs(real_B - cycle_B))\n",
    "\n",
    "                    # Generator Identity Loss\n",
    "                    identityLoss = torch.mean(\n",
    "                        torch.abs(real_A - identity_A)) + torch.mean(torch.abs(real_B - identity_B))\n",
    "\n",
    "                    # Generator Loss\n",
    "                    g_loss_A2B = torch.mean((1 - d_fake_B) ** 2)\n",
    "                    g_loss_B2A = torch.mean((1 - d_fake_A) ** 2)\n",
    "\n",
    "                    # Generator Two Step Adverserial Loss\n",
    "                    generator_loss_A2B_2nd = torch.mean((1 - d_fake_cycle_B) ** 2)\n",
    "                    generator_loss_B2A_2nd = torch.mean((1 - d_fake_cycle_A) ** 2)\n",
    "\n",
    "                    # Total Generator Loss\n",
    "                    g_loss = g_loss_A2B + g_loss_B2A + \\\n",
    "                        generator_loss_A2B_2nd + generator_loss_B2A_2nd + \\\n",
    "                        self.cycle_loss_lambda * cycleLoss + self.identity_loss_lambda * identityLoss\n",
    "                    \n",
    "                    if j % 10 == 0:\n",
    "                        print(\"loss:\", g_loss)\n",
    "\n",
    "                    # Backprop for Generator\n",
    "                    self.reset_grad()\n",
    "                    g_loss.backward()\n",
    "                    self.generator_optimizer.step()\n",
    "\n",
    "                    # ----------------------\n",
    "                    # Train Discriminator\n",
    "                    # ----------------------\n",
    "                    self.generator_A2B.eval()\n",
    "                    self.generator_B2A.eval()\n",
    "                    self.discriminator_A.train()\n",
    "                    self.discriminator_B.train()\n",
    "                    self.discriminator_A2.train()\n",
    "                    self.discriminator_B2.train()\n",
    "\n",
    "                    # Discriminator Feed Forward\n",
    "                    d_real_A = self.discriminator_A(real_A)\n",
    "                    d_real_B = self.discriminator_B(real_B)\n",
    "                    d_real_A2 = self.discriminator_A2(real_A)\n",
    "                    d_real_B2 = self.discriminator_B2(real_B)\n",
    "                    generated_A = self.generator_B2A(real_B, mask_B)\n",
    "                    d_fake_A = self.discriminator_A(generated_A)\n",
    "\n",
    "                    # For Two Step Adverserial Loss A->B\n",
    "                    cycled_B = self.generator_A2B(\n",
    "                        generated_A, torch.ones_like(generated_A))\n",
    "                    d_cycled_B = self.discriminator_B2(cycled_B)\n",
    "\n",
    "                    generated_B = self.generator_A2B(real_A, mask_A)\n",
    "                    d_fake_B = self.discriminator_B(generated_B)\n",
    "\n",
    "                    # For Two Step Adverserial Loss B->A\n",
    "                    cycled_A = self.generator_B2A(\n",
    "                        generated_B, torch.ones_like(generated_B))\n",
    "                    d_cycled_A = self.discriminator_A2(cycled_A)\n",
    "\n",
    "                    # Loss Functions\n",
    "                    d_loss_A_real = torch.mean((1 - d_real_A) ** 2)\n",
    "                    d_loss_A_fake = torch.mean((0 - d_fake_A) ** 2)\n",
    "                    d_loss_A = (d_loss_A_real + d_loss_A_fake) / 2.0\n",
    "\n",
    "                    d_loss_B_real = torch.mean((1 - d_real_B) ** 2)\n",
    "                    d_loss_B_fake = torch.mean((0 - d_fake_B) ** 2)\n",
    "                    d_loss_B = (d_loss_B_real + d_loss_B_fake) / 2.0\n",
    "\n",
    "                    # Two Step Adverserial Loss\n",
    "                    d_loss_A_cycled = torch.mean((0 - d_cycled_A) ** 2)\n",
    "                    d_loss_B_cycled = torch.mean((0 - d_cycled_B) ** 2)\n",
    "                    d_loss_A2_real = torch.mean((1 - d_real_A2) ** 2)\n",
    "                    d_loss_B2_real = torch.mean((1 - d_real_B2) ** 2)\n",
    "                    d_loss_A_2nd = (d_loss_A2_real + d_loss_A_cycled) / 2.0\n",
    "                    d_loss_B_2nd = (d_loss_B2_real + d_loss_B_cycled) / 2.0\n",
    "\n",
    "                    # Final Loss for discriminator with the Two Step Adverserial Loss\n",
    "                    d_loss = (d_loss_A + d_loss_B) / 2.0 + \\\n",
    "                        (d_loss_A_2nd + d_loss_B_2nd) / 2.0\n",
    "\n",
    "                    # Backprop for Discriminator\n",
    "                    self.reset_grad()\n",
    "                    d_loss.backward()\n",
    "                    self.discriminator_optimizer.step()\n",
    "\n",
    "                j += 1\n",
    "                # Log Iteration\n",
    "                # self.logger.log_iter(\n",
    "                #     loss_dict={'g_loss': g_loss.item(), 'd_loss': d_loss.item()})\n",
    "                # self.logger.end_iter()\n",
    "\n",
    "                # # Adjust learning rates\n",
    "                # if self.logger.global_step > self.decay_after:\n",
    "                #     self.adjust_lr_rate(\n",
    "                #         self.generator_optimizer, generator=True)\n",
    "                #     self.adjust_lr_rate(\n",
    "                #         self.generator_optimizer, generator=False)\n",
    "\n",
    "                # # Set identity loss to zero if larger than given value\n",
    "                # if self.logger.global_step > self.stop_identity_after:\n",
    "                #     self.identity_loss_lambda = 0\n",
    "\n",
    "            # # Log intermediate outputs on Tensorboard\n",
    "            # if self.logger.epoch % self.epochs_per_plot == 0:\n",
    "            #     with torch.no_grad():\n",
    "            #         # Log Mel-spectrograms .png\n",
    "            #         real_mel_A_fig = get_mel_spectrogram_fig(\n",
    "            #             real_A[0].detach().cpu())\n",
    "            #         fake_mel_A_fig = get_mel_spectrogram_fig(\n",
    "            #             generated_A[0].detach().cpu())\n",
    "            #         real_mel_B_fig = get_mel_spectrogram_fig(\n",
    "            #             real_B[0].detach().cpu())\n",
    "            #         fake_mel_B_fig = get_mel_spectrogram_fig(\n",
    "            #             generated_B[0].detach().cpu())\n",
    "            #         self.logger.visualize_outputs({\"real_A_spec\": real_mel_A_fig, \"fake_B_spec\": fake_mel_B_fig,\n",
    "            #                                        \"real_B_spec\": real_mel_B_fig, \"fake_A_spec\": fake_mel_A_fig})\n",
    "\n",
    "            #         # Convert Mel-spectrograms from validation set to waveform and log to tensorboard\n",
    "            #         real_mel_full_A, real_mel_full_B = next(\n",
    "            #             iter(self.validation_dataloader))\n",
    "            #         real_mel_full_A = real_mel_full_A.to(\n",
    "            #             self.device, dtype=torch.float)\n",
    "            #         real_mel_full_B = real_mel_full_B.to(\n",
    "            #             self.device, dtype=torch.float)\n",
    "            #         fake_mel_full_B = self.generator_A2B(\n",
    "            #             real_mel_full_A, torch.ones_like(real_mel_full_A))\n",
    "            #         fake_mel_full_A = self.generator_B2A(\n",
    "            #             real_mel_full_B, torch.ones_like(real_mel_full_B))\n",
    "            #         real_wav_full_A = decode_melspectrogram(self.vocoder, real_mel_full_A[0].detach(\n",
    "            #         ).cpu(), self.dataset_A_mean, self.dataset_A_std).cpu()\n",
    "            #         fake_wav_full_A = decode_melspectrogram(self.vocoder, fake_mel_full_A[0].detach(\n",
    "            #         ).cpu(), self.dataset_A_mean, self.dataset_A_std).cpu()\n",
    "            #         real_wav_full_B = decode_melspectrogram(self.vocoder, real_mel_full_B[0].detach(\n",
    "            #         ).cpu(), self.dataset_B_spec_mean, self.dataset_B_spec_std).cpu()\n",
    "            #         fake_wav_full_B = decode_melspectrogram(self.vocoder, fake_mel_full_B[0].detach(\n",
    "            #         ).cpu(), self.dataset_B_mean, self.dataset_B_spec_std).cpu()\n",
    "            #         self.logger.log_audio(\n",
    "            #             real_wav_full_A.T, \"real_speaker_A_audio\", self.sample_rate)\n",
    "            #         self.logger.log_audio(\n",
    "            #             fake_wav_full_A.T, \"fake_speaker_A_audio\", self.sample_rate)\n",
    "            #         self.logger.log_audio(\n",
    "            #             real_wav_full_B.T, \"real_speaker_B_audio\", self.sample_rate)\n",
    "            #         self.logger.log_audio(\n",
    "            #             fake_wav_full_B.T, \"fake_speaker_B_audio\", self.sample_rate)\n",
    "\n",
    "            # Save each model checkpoint\n",
    "            # if self.logger.epoch % self.epochs_per_save == 0:\n",
    "            #     self.saver.save(self.logger.epoch, self.generator_A2B,\n",
    "            #                     self.generator_optimizer, None, args.device, \"generator_A2B\")\n",
    "            #     self.saver.save(self.logger.epoch, self.generator_B2A,\n",
    "            #                     self.generator_optimizer, None, args.device, \"generator_B2A\")\n",
    "            #     self.saver.save(self.logger.epoch, self.discriminator_A,\n",
    "            #                     self.discriminator_optimizer, None, args.device, \"discriminator_A\")\n",
    "            #     self.saver.save(self.logger.epoch, self.discriminator_B,\n",
    "            #                     self.discriminator_optimizer, None, args.device, \"discriminator_B\")\n",
    "            #     self.saver.save(self.logger.epoch, self.discriminator_A2,\n",
    "            #                     self.discriminator_optimizer, None, args.device, \"discriminator_A2\")\n",
    "            #     self.saver.save(self.logger.epoch, self.discriminator_B2,\n",
    "            #                     self.discriminator_optimizer, None, args.device, \"discriminator_B2\")\n",
    "\n",
    "            # self.logger.end_epoch()\n",
    "            end_epoch_time = time.time()\n",
    "        end_time = time.time()\n",
    "\n",
    "        self.training_time = end_time - start_time\n",
    "        self.average_time = self.training_time / (self.num_epochs - self.start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycleGAN = MaskCycleGANVCTraining()\n",
    "cycleGAN.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS228",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
